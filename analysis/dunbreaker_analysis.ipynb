{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:12:10.717157Z",
     "start_time": "2024-12-05T20:12:09.500106Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from collections import defaultdict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def convert_img_grayscale(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the images specified in the 'img_path' column to grayscale, resizes them to 64x64.\n",
    "    Returns a DataFrame with 'uuid' and the flattened pixel values.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with columns 'uuid' and 'img_path'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with 'uuid' and columns for each pixel in the image.\n",
    "    \"\"\"\n",
    "    # List to store uuid and flattened image data\n",
    "    uuid_list = []\n",
    "    pixel_values_list = []\n",
    "\n",
    "    # Loop through each row in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = row['img_path']\n",
    "        uuid = row['uuid']\n",
    "\n",
    "        try:\n",
    "            # Open the image using Pillow\n",
    "            with Image.open(img_path) as img:\n",
    "                # Resize the image to 64x64\n",
    "                img_resized = img.resize((64, 64))\n",
    "\n",
    "                # Convert the image to grayscale\n",
    "                img_gray = img_resized.convert(\"L\")  # 'L' mode stands for grayscale\n",
    "\n",
    "                # img_gray.show()\n",
    "                # Convert the grayscale image to a numpy array\n",
    "                img_array = np.array(img_gray)\n",
    "\n",
    "                # Flatten the 2D array to 1D\n",
    "                img_flattened = img_array.flatten()\n",
    "\n",
    "                # Append the results to the lists\n",
    "                uuid_list.append(uuid)\n",
    "                pixel_values_list.append(img_flattened)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {e}\")\n",
    "            # If error occurs, add None for the image\n",
    "            uuid_list.append(uuid)\n",
    "            pixel_values_list.append([None] * 4096)  # Assuming a 64x64 image (4096 pixels)\n",
    "\n",
    "    # Create a new DataFrame with 'uuid' and the flattened image data\n",
    "    pixel_columns = [f'pixel_{i + 1}' for i in range(4096)]  # 64x64 = 4096 pixels\n",
    "    result_df = pd.DataFrame(pixel_values_list, columns=pixel_columns)\n",
    "    result_df['uuid'] = uuid_list  # Add 'uuid' column at the end\n",
    "\n",
    "    # Reorder columns to have 'uuid' as the first column\n",
    "    result_df = result_df[['uuid'] + pixel_columns]\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def preprocess_img_file(image_folder: str, image_files: list, resize: bool = False) -> pd.DataFrame:\n",
    "    # Use a defaultdict to group files by prefix\n",
    "    prefix_dict = defaultdict(list)\n",
    "    unique_img_dic = {}\n",
    "\n",
    "    # create a uuid dictionary with uuid as key and it's otolith images as values\n",
    "    for image_file in image_files:\n",
    "        # Extract the UUID from the image filename\n",
    "        uuid = image_file.split('_')[0]  # Get the part before \"_\"\n",
    "\n",
    "        prefix_dict[uuid].append(image_file)\n",
    "\n",
    "    for uuid, images in prefix_dict.items():\n",
    "        # Get the first image for each prefix\n",
    "        if images:\n",
    "            first_image = images[0]\n",
    "\n",
    "            # Construct the full image path\n",
    "            if uuid not in unique_img_dic:\n",
    "                image_path = os.path.join(image_folder, first_image)\n",
    "                unique_img_dic[uuid] = image_path\n",
    "\n",
    "    unique_img_df = pd.DataFrame(list(unique_img_dic.items()), columns=['uuid', 'img_path'])\n",
    "\n",
    "    gs_img = convert_img_grayscale(unique_img_df)\n",
    "\n",
    "    return gs_img\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:21:25.898774Z",
     "start_time": "2024-12-05T20:20:08.646143Z"
    }
   },
   "source": [
    "# Path to your image folder\n",
    "image_folder = os.path.join(\"..\", \"data\", \"images\")\n",
    "\n",
    "# 1. List all image files in the folder\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "# 2. Randomly select 1000 images from the folder\n",
    "# sampled_images = random.sample(image_files, 1000)\n",
    "\n",
    "# 3. read meta data\n",
    "meta_data = pd.read_csv(os.path.join(\"..\", \"data\", \"metadata.csv\"))\n",
    "\n",
    "# 4.preprocess the images: randomly select the first image of each otolith and convert to grayscale\n",
    "gs_img = preprocess_img_file(image_folder, image_files)\n",
    "\n",
    "# 5.join the grayscale dataframe with the metadata by uuid\n",
    "merged_df = pd.merge(meta_data, gs_img, on='uuid', how='left')\n",
    "\n",
    "# 6.Separate features (X) and target (y)\n",
    "pixel_columns = [f'pixel_{i + 1}' for i in range(4096)]\n",
    "# cat_vars = ['is_male', 'is_female', 'is_unknown', 'is_plaice', 'is_herring']\n",
    "cat_vars = ['is_plaice', 'is_herring']\n",
    "# num_vars = ['length', 'weight', 'month'] + pixel_columns\n",
    "num_vars = pixel_columns\n",
    "\n",
    "feature_cols = cat_vars + num_vars\n",
    "\n",
    "X = merged_df[feature_cols]\n",
    "y = merged_df['age']  # Target variable\n",
    "\n",
    "# 7.Create a ColumnTransformer\n",
    "# Numerical Features: We'll scale them using StandardScaler.\n",
    "# Categorical Features: We'll encode them using OneHotEncoder.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_vars),  # Standard scaling for numerical features\n",
    "    ])\n",
    "# 8.Fit and transform the dateset (Standard scaling for numerical features and One-hot encoding for categorical features)\n",
    "X = preprocessor.fit_transform(X)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:21:49.831287Z",
     "start_time": "2024-12-05T20:21:49.656176Z"
    }
   },
   "source": [
    "# 9.select first 6000 instances for train and test\n",
    "X_train = X[:6000]\n",
    "y_train = y[:6000]\n",
    "\n",
    "# select remaining 1828 instances for validation\n",
    "X_val = X[6000:]\n",
    "y_val = y[6000:]\n",
    "\n",
    "# 10. Split the dataset into training and testing sets\n",
    "X_train_sub, X_test, y_train_sub, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:27:47.881809Z",
     "start_time": "2024-12-05T20:21:56.763357Z"
    }
   },
   "source": [
    "# 1. Train a Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "\n",
    "rf.fit(X_train_sub, y_train_sub)\n",
    "y_pred = rf.predict(X_test)\n",
    "# 11. Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 4.79\n",
      "Root Mean Squared Error: 2.187610340074301\n",
      "R-squared (R²): 0.52\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:28:14.823576Z",
     "start_time": "2024-12-05T20:28:14.778337Z"
    }
   },
   "source": [
    "# 2. Evaluate the model by the remaining 1828 instances\n",
    "y_val_pred = rf.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.31972932166302\n",
      "Root Mean Squared Error: 1.8220124372964692\n",
      "R-squared (R²): 0.28\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:28:31.935709Z",
     "start_time": "2024-12-05T20:28:22.504468Z"
    }
   },
   "source": [
    "# 1. Train a Random Forest Regressor\n",
    "dtrain = xgb.DMatrix(X_train_sub, label=y_train_sub)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Regression objective using squared error\n",
    "    'max_depth': 6,  # Depth of the trees\n",
    "    'learning_rate': 0.1,  # Step size (shrinkage)\n",
    "    'eval_metric': 'rmse'  # Evaluation metric - RMSE (Root Mean Squared Error)\n",
    "}\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=50)\n",
    "y_pred = xgb_model.predict(dtest)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4.161688090761637\n",
      "Root Mean Squared Error: 2.040021590758695\n",
      "R-squared (R²): 0.58\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:28:41.524085Z",
     "start_time": "2024-12-05T20:28:41.481734Z"
    }
   },
   "source": [
    "# 2. Evaluate the model by the remaining 1828 instances\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "y_val_xgb_pred = xgb_model.predict(dval)\n",
    "\n",
    "mse = mean_squared_error(y_val, y_val_xgb_pred)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_val, y_val_xgb_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.981805855634052\n",
      "Root Mean Squared Error: 1.7267906229864847\n",
      "R-squared (R²): 0.35\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:29:46.521886Z",
     "start_time": "2024-12-05T20:28:46.520522Z"
    }
   },
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train_sub, _, y_train_sub, _ = train_test_split(X_train, y_train, train_size=0.8, random_state=42)\n",
    "dtrain = xgb.DMatrix(X_train_sub, label=y_train_sub)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Instantiate XGBoost regressor\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "param = {\n",
    "    'objective': 'reg:squarederror',  # Regression task\n",
    "    'eval_metric': 'rmse',  # Root Mean Squared Error for evaluation\n",
    "    'max_depth': 6,  # Maximum depth of the trees\n",
    "    'learning_rate': 0.1,  # Learning rate\n",
    "}\n",
    "\n",
    "# Perform cross-validation to find the best number of boosting rounds\n",
    "cv_results = xgb.cv(\n",
    "    params=param,  # Hyperparameters\n",
    "    dtrain=dtrain,  # Training data\n",
    "    num_boost_round=100,  # Maximum number of boosting rounds\n",
    "    nfold=3,  # 5-fold cross-validation\n",
    "    metrics=\"rmse\",  # Evaluation metric (RMSE)\n",
    "    early_stopping_rounds=15,  # Stop if no improvement after 10 rounds\n",
    "    as_pandas=True,  # Return results as a pandas DataFrame\n",
    "    seed=42  # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Get the best number of boosting rounds (the best iteration)\n",
    "best_num_boost_round = cv_results['test-rmse-mean'].idxmin()\n",
    "\n",
    "# Now train the final model using the best number of boosting rounds\n",
    "final_model = xgb.train(\n",
    "    params=param,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=best_num_boost_round\n",
    ")\n",
    "\n",
    "y_pred = final_model.predict(dtest)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.8582654849831313\n",
      "Root Mean Squared Error: 1.9642467983894314\n",
      "R-squared (R²): 0.61\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-05T20:29:56.603628Z"
    }
   },
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create and train a Gradient Boosting Regressor\n",
    "model = GradientBoostingRegressor(n_estimators=100)\n",
    "model.fit(X_train_sub, y_train_sub)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T20:29:51.781447Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Create base models\n",
    "base_learners = [\n",
    "    ('svr', SVR()),\n",
    "    ('dt', DecisionTreeRegressor())\n",
    "]\n",
    "\n",
    "# Create a meta-model\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "# Create and train the Stacking Regressor\n",
    "model = StackingRegressor(estimators=base_learners, final_estimator=meta_model)\n",
    "model.fit(X_train_sub, y_train_sub)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T20:29:51.784448200Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T20:29:51.787444900Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(X_train, X_test, y_train, y_test):\n",
    "    train_sizes = [0.4, 0.6, 0.8, 1]  # Training sizes from 10% to 100% of the training data\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    cv_results_dic = {}\n",
    "    for train_size in train_sizes:\n",
    "        # Create a subset of the training data based on the current train_size\n",
    "        X_train_sub, _, y_train_sub, _ = train_test_split(X_train, y_train, train_size=train_size, random_state=42)\n",
    "        dtrain = xgb.DMatrix(X_train_sub, label=y_train_sub)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "        param = {\n",
    "            'objective': 'reg:squarederror',  # Regression objective using squared error\n",
    "            'max_depth': 6,  # Depth of the trees\n",
    "            'learning_rate': 0.1,  # Step size (shrinkage)\n",
    "            'eval_metric': 'rmse'  # Evaluation metric - RMSE (Root Mean Squared Error)\n",
    "        }\n",
    "\n",
    "        # Step 5: Perform k-fold cross-validation using xgb.cv\n",
    "\n",
    "        print(f\"Best number of boosting rounds: {best_num_boost_round}\")\n",
    "        # Step 8: Train the model with the best boosting rounds\n",
    "        bst = xgb.train(param, dtrain, num_boost_round=best_num_boost_round)\n",
    "\n",
    "        # Train error\n",
    "        train_pred = bst.predict(dtrain)\n",
    "        train_score = mean_squared_error(y_train_sub, train_pred)\n",
    "\n",
    "        # Validation error\n",
    "        val_pred = bst.predict(dtest)\n",
    "        test_score = mean_squared_error(y_test, val_pred)\n",
    "\n",
    "        # Append the errors for plotting\n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "\n",
    "    return train_scores, test_scores, cv_results_dic\n",
    "\n",
    "\n",
    "# Call the function to plot the learning curve\n",
    "train_scores, test_scores, cv_results = plot_learning_curve(X_train, X_test, y_train, y_test)\n",
    "train_sizes = [0.4, 0.6, 0.8, 1]  # Training sizes from 10% to 100% of the training data\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_scores, label='Training score', color='blue')\n",
    "plt.plot(train_sizes, test_scores, label='validation score', color='green')\n",
    "# Set y-axis limits from 0 to 20\n",
    "plt.ylim(0, 5)\n",
    "# Show the ticks and their labels on y-axis (you can adjust the tick range based on data)\n",
    "plt.yticks(np.arange(0, 5, 0.2))  # Show ticks from 0 to 20 with a step of 2\n",
    "\n",
    "# Labels and title\n",
    "plt.title('Learning Curve for XGBoost Model')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN- Below is an example of building and training a CNN model for a regression model using TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T20:29:51.789447500Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "X = merged_df[pixel_columns]\n",
    "y = merged_df['age'] + merged_df['month']  # Target variable\n",
    "\n",
    "X_train = X[:6000]\n",
    "y_train = y[:6000]\n",
    "\n",
    "# select remaining 1828 instances for final testing\n",
    "X_val = X[6000:]\n",
    "y_val = y[6000:]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_sub, X_test, y_train_sub, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the image data (important for CNNs)\n",
    "X_train_sub = X_train_sub / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_val = X_val / 255.0\n",
    "\n",
    "# Build the CNN model for regression\n",
    "model = Sequential()\n",
    "\n",
    "# Reshape the data to add the channel dimension (28x28 images with 1 channel for grayscale)\n",
    "height = 64\n",
    "width = 64\n",
    "channels = 1\n",
    "X_train_sub = X_train_sub.values.reshape(X_train_sub.shape[0], height, width, channels)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], height, width, channels)\n",
    "X_val = X_val.values.reshape(X_val.shape[0], height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T20:29:51.791448400Z"
    }
   },
   "outputs": [],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T20:29:51.793445200Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "# Add convolutional layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer=HeNormal(), input_shape=(height, width, channels)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the output of the convolutional layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected (dense) layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "\n",
    "# Output layer for regression: One neuron, no activation function (linear activation)\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Step 4: Set up Early Stopping callback to monitor validation loss\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_sub, y_train_sub, epochs=100, batch_size=64, callbacks=[early_stopping],\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 7: Get the best epoch from the early stopping\n",
    "best_epoch = np.argmin(history.history['val_loss']) + 1  # Adding 1 because epochs start from 1\n",
    "print(f\"Best Epoch (based on validation loss): {best_epoch}\")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = history.model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) and RMSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T20:29:51.795443900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred = history.model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) and RMSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T20:29:51.797445400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_val_pred = history.model.predict(X_val)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) and RMSE\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T20:29:51.798442600Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# Set y-axis limits from 0 to 20\n",
    "plt.ylim(0, 5)\n",
    "# Show the ticks and their labels on y-axis (you can adjust the tick range based on data)\n",
    "plt.yticks(np.arange(0, 5, 0.5))  # Show ticks from 0 to 20 with a step of 2\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
